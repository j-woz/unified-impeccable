= unified-impeccable

== Steps

Step 1 "HTP Docking"::
Code is `/htp_docking`
+
`sub_p1_s1`: WALLTIME=20m
+
* Frontier: success in <22 minutes
* Aurora:   success in <?? minutes

Step 2: "Surrogate Training"::
Code is `/surrogate_training`
+
`sub_p1_s2`: - WALLTIME=1h
+
* Frontier: success in <10 minutes
* Aurora:   blocked on TF error- Archit looking at it

Step 3: "Surrogate Inference"::
Code is `/surrogate_inference`
+
* Frontier: `sub_p1_s3` - WALLTIME=1h - success in <10 minutes

Step 4a: "Pose Generation"::
Code is `/pose_generation`
+
* Frontier: `sub_p2_s4a` - WALLTIME=1h - success in <2 minutes

== Script architecture

=== Install time

* `install-py-pkgs.sh`

=== Naming conventions

`CODE_DIR`::
The original location of the code, typically a git clone

`WORK_DIR`::
The code is copied here for execution

`MEM_DIR`::
A subdirectory of `WORK_DIR` for step data.  This is the `PWD` for the science code execution

`THIS`::
The directory containing the current script.  On Frontier/Slurm, this is found with a Slurm-specific variable.  On Aurora/PBS, we can use `PBS_O_WORKDIR`.  But it is better to look this up for each run script for interactive cases where the user may `cd` from the original submit-time directory.

`WORKFLOW_DIR`::
The top-level directory `unified-impeccable/workflow`

=== Workflow-level scripts

* `site-SITE-settings.sh`
** General Software
*** Sets up non-Python software (licenses, schedulers)
** Python Settings
*** Loads Anaconda, etc.
** User Settings
*** Specific to the user that cloned the workflow.  User must specify the location of the IMPECCABLE clone and a desired output directory.
* `impeccable-settings.sh`
** Application-level settings for a specific science run.
** User may edit this to set `SMILES_INPUT`, `RECEP_FILE`, `INFERENCE_DATA` locations, etc.

== Typical user scenario

. Clone IMPECCABLE
. Clone this workflow repo
. Set up Python and run `install-py-pkgs.sh`
. Copy
.. `site-SITE-template.sh` to `site-SITE-settings.sh`
.. `impeccable-template.sh` to `impeccable-settings.sh`
.. Thus, `*-settings.sh` files are local and should not be pushed to git.
. Edit `setup-SITE.sh` for license and any other software, including Python
. Obtain inference data
.. On Frontier, at: +
`/lustre/orion/chm155/proj-shared/apbhati/InferenceData` +
You can point to this or make a copy.
. Edit `impeccable-settings.sh` to set up the `INFERENCE_DATA` location, etc.
. Start step with `stepN/submit-SITE.sh`

== Step progress model

. User invokes `stepN/submit-SITE.sh` .  This submits the job to the scheduler.
. This runs `stepN/sub_pP_sN-SITE.sh` ,
.. which refers to `../site-SITE-settings.sh` for site settings and
.. loads application settings from `../impeccable-settings.sh`
. `sub_pP_sN-SITE.sh`:
.. sources `sub_pP_sN-setup.sh`, whicih:
... Sets up `CODE_DIR`, `WORK_DIR`, `MEM_DIR`
... Copies software to `WORK_DIR`
... Filters input files via m4 (e.g., JSON)
.. contains the SITE-specific scheduler settings and
.. invokes the science codes via `mpiexec`, `srun`, etc.

== Anaconda environments

We call this `CONDA_ENVIRONMENT`.

* step1: oepython_new
* step2: st_train
* step3: st_mpi_base

This is specified for each step by `sub_pP_sN-SITE.sh` , which passes it into the `sub_pP_sN-setup.sh` script, which passes it to `site-SITE-settings.sh`, which activates that environment after loading Python for that site.

=== Frontier

=== Aurora

Using Miniconda in $HOME due to Lustre situation.  ~wozniak/Public/sfw/Miniconda-312-IMPECC

== TODO

=== Justin

Insert DB commands

Create settings file with:
  PROJECT/ACCOUNT
  IMPECCABLE location - DONE

Per-campaign job directory
  EXP001, EXP002, ...

Output stream from job should go into job output directory

PSI/J

=== Ketan

Try step4
